{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-weight: 400;\">Our financial institution, <em>Davis-Charles</em> <em>Investments</em> is responsible for managing customers&rsquo; financial accounts. Similarly to all financial institutions <strong>credibility</strong> is one of the most core aspects of the business. This is why for many years the company relied on 3rd party services to detect and stop fraudulent transactions. However, in recent years managers at Davis-Charles learned about two domains called \"Machine Learning\" and \"Data Science\". After consulting with some advisors, they have decided to set up an internal data science team that will be responsible for implementing ML related solutions, starting with <strong>reducing the number of fraud cases associated with financial transactions</strong> that damage the company&rsquo;s reputation and inflict financial losses.&nbsp;</span></p>\n",
    "<p><span style=\"font-weight: 400;\">The team leader, Dr. Aaliyah Jefferson is an expert of Machine Learning that doesn't have a strong background in cloud services and development. A consultant advised the management and the team to implement their solution on <span style=\"color: #ff6600;\"><strong>AWS</strong></span> since the platform has a very strong ecosystem for these cases exactly called <strong><span style=\"color: #ff6600;\">SageMaker</span></strong>.&nbsp;</span></p>\n",
    "<p>&nbsp;</p>\n",
    "<p><span style=\"font-weight: 400;\">Together with the core-data team ad Davis-Charles Aaliyah strives to get her hands on a dataset of samples in order to start the research phase.&nbsp;</span></p>\n",
    "<p><span style=\"font-weight: 400;\">For the&nbsp; core-data team it&rsquo;s very obvious that they should use <strong><span style=\"color: #ff6600;\">Redshift</span></strong> as the data warehouse for this project. They start by creating a table and loading data that they have collected from the company&rsquo;s on-prem DB.</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helpers import get_conf, get_engine\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_conf()\n",
    "engine = get_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = conf['DATA_BUCKET']\n",
    "redshift_iam_role = conf['REDSHIFT_IAM_ROLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_file_path = f's3://{data_bucket}/raw_data/PS_20174392719_1491204439457_log.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(s3_file_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"raw_data\" (\n",
      "\"step\" INTEGER,\n",
      "  \"type\" TEXT,\n",
      "  \"amount\" REAL,\n",
      "  \"nameOrig\" TEXT,\n",
      "  \"oldbalanceOrg\" REAL,\n",
      "  \"newbalanceOrig\" REAL,\n",
      "  \"nameDest\" TEXT,\n",
      "  \"oldbalanceDest\" REAL,\n",
      "  \"newbalanceDest\" REAL,\n",
      "  \"isFraud\" INTEGER,\n",
      "  \"isFlaggedFraud\" INTEGER\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, 'raw_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Redshift and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_load_data = f'''\n",
    "copy raw_data from  {s3_file_path}\n",
    "iam_role {redshift_iam_role}\n",
    "IGNOREHEADER 1 DELIMITER ',' ;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = engine.connect()\n",
    "results = connection.execute(text(sql_load_data))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
